{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargando los datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.classification_dataset import DermaClassificationDataset\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def get_dataset_image_root() -> Path:\n",
    "    \"\"\"Returns the path to the root of the classification dataset\"\"\"\n",
    "    return Path(\"./images/classification//\")\n",
    "\n",
    "def get_classification_csv_path() -> Path:\n",
    "    \"\"\"Returns the path to the csv file containing the information about the images\n",
    "    and their classifications\"\"\"\n",
    "    return Path(\"./datasets/csv_files/clean_classification_dataset.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training images: 42180\n",
      "Size of validation images: 10546\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "all_images_df = pd.read_csv(get_classification_csv_path())\n",
    "train_files, val_files = train_test_split(all_images_df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Size of training images: {len(train_files)}\")\n",
    "print(f\"Size of validation images: {len(val_files)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.dataset_transforms import (\n",
    "    get_classification_train_transforms_v2,\n",
    "    get_classification_evaluation_transform_v2,\n",
    ")\n",
    "\n",
    "train_transform = get_classification_train_transforms_v2(\n",
    "    image_size=(224, 224), dull_razor_probability=0.5\n",
    ")\n",
    "val_transform = get_classification_evaluation_transform_v2()\n",
    "\n",
    "train_dataset = DermaClassificationDataset(\n",
    "    root_img_folder=get_dataset_image_root(),\n",
    "    image_dataframe=train_files,\n",
    "    transform=train_transform,\n",
    ")\n",
    "\n",
    "val_dataset = DermaClassificationDataset(\n",
    "    root_img_folder=get_dataset_image_root(),\n",
    "    image_dataframe=val_files,\n",
    "    transform=val_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creando los dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[32m██████████\u001b[0m| 42180/42180 [00:00<00:00, 629866.86it/s]\n",
      "100%|\u001b[32m██████████\u001b[0m| 10546/10546 [00:00<00:00, 932519.50it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dl = train_dataset.get_balanced_dataloader(batch_size=32, num_workers=8)\n",
    "\n",
    "val_dl = val_dataset.get_balanced_dataloader(batch_size=32, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def get_image_counts_by_category(dataloader):\n",
    "    \"\"\"\n",
    "    Returns a dictionary with the count of images in each category from the given DataLoader.\n",
    "\n",
    "    Args:\n",
    "        dataloader (DataLoader): The DataLoader to analyze.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where each key is a category (as an integer) and the value is the total count of images in that category.\n",
    "    \"\"\"\n",
    "    category_counts = {}\n",
    "\n",
    "    # Iterate over the DataLoader\n",
    "    for batch in tqdm(dataloader, colour=\"blue\", leave=True):\n",
    "        _, labels = batch\n",
    "\n",
    "        # Iterate over the labels in the batch\n",
    "        for label in labels:\n",
    "            label = label.item()  # Convert tensor to integer\n",
    "            if label in category_counts:\n",
    "                category_counts[label] += 1\n",
    "            else:\n",
    "                category_counts[label] = 1\n",
    "\n",
    "    return category_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1319/1319 [05:20<00:00,  4.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first dataloader have the following distribution:\n",
      "\tdict_items([(1, 10384), (0, 10600), (3, 10579), (2, 10617)])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "first = get_image_counts_by_category(train_dl)\n",
    "\n",
    "print(f\"The dataloader have the following distribution:\\n\\t{first.items()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, batch in tqdm(enumerate(train_dl), desc=\"Checking speed of the dataloader\", colour=\"blue\"):\n",
    "#     continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.resnet_transfer_learning import get_resnet50\n",
    "import torchsummary\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, loss, optimizer = get_resnet50(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchsummary.summary(\n",
    "    model,\n",
    "    input_data=(3,224,224)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones de utilidad para el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(x, y, model, optimizer, criteria):\n",
    "    \"\"\"Train a batch of data and return the loss value and accuracy\"\"\"\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    prediction = model(x)\n",
    "    batch_loss = criteria(prediction, y)\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    correct = prediction.argmax(dim=1).eq(y).sum().item()\n",
    "    return batch_loss.item(), correct\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_batch(x, y, model, criteria):\n",
    "    \"\"\"Validate a batch of data and return the loss value and the number of correct predictions\"\"\"\n",
    "    model.eval()\n",
    "    prediction = model(x)\n",
    "    batch_loss = criteria(prediction, y)\n",
    "    correct = prediction.argmax(dim=1).eq(y).sum().item()\n",
    "    return batch_loss.item(), correct\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epochs(\n",
    "    model, optimizer, criteria, train_dl, val_dl, epochs=10, device=\"cuda\"\n",
    "):\n",
    "    total_train_loss = 0\n",
    "    total_train_accuracy = 0\n",
    "    total_val_loss = 0\n",
    "    total_val_accuracy = 0\n",
    "    \"\"\"Train the model for a number of epochs\"\"\"\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for index, batch in tqdm(enumerate(train_dl), total=len(train_dl), desc=f\"Training epoch {epoch}\", colour=\"red\"):\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            loss, accuracy = train_batch(x, y, model, optimizer, criteria)\n",
    "            total_train_loss += loss\n",
    "            total_train_accuracy += accuracy\n",
    "\n",
    "        print(f\"Training Loss: {total_train_loss/len(train_dl)}\")\n",
    "        print(f\"Training Accuracy: {total_train_accuracy/len(train_dl)}\")\n",
    "\n",
    "        for index, batch in tqdm(enumerate(val_dl), total=len(val_dl), desc=f\"Validation on {epoch}\", colour=\"green\"):\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            loss, accuracy = validate_batch(x, y, model, criteria)\n",
    "            total_val_loss += loss\n",
    "            total_val_accuracy += accuracy\n",
    "        \n",
    "        print(f\"Validation Loss: {total_val_loss/len(val_dl)}\")\n",
    "        print(f\"Validation Accuracy: {total_val_accuracy/len(val_dl)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:  72%|\u001b[31m███████▏  \u001b[0m| 951/1319 [12:57<05:00,  1.22it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_epochs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 15\u001b[0m, in \u001b[0;36mtrain_epochs\u001b[0;34m(model, optimizer, criteria, train_dl, val_dl, epochs, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m x, y \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     14\u001b[0m x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 15\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriteria\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     17\u001b[0m total_train_accuracy \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m accuracy\n",
      "Cell \u001b[0;32mIn[17], line 9\u001b[0m, in \u001b[0;36mtrain_batch\u001b[0;34m(x, y, model, optimizer, criteria)\u001b[0m\n\u001b[1;32m      7\u001b[0m batch_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m----> 9\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[43mprediction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meq\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch_loss\u001b[38;5;241m.\u001b[39mitem(), correct\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_epochs(\n",
    "    model,\n",
    "    optimizer,\n",
    "    loss,\n",
    "    train_dl,\n",
    "    val_dl,\n",
    "    epochs=3,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_ai_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
